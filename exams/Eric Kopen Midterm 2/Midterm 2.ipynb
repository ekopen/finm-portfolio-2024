{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559895d2",
   "metadata": {},
   "source": [
    "# Midterm 2\n",
    "\n",
    "## FINM 36700 - 2024\n",
    "\n",
    "### UChicago Financial Mathematics\n",
    "\n",
    "* Mark Hendricks\n",
    "* hendricks@uchicago.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cde8d3",
   "metadata": {},
   "source": [
    "# Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc273c1a",
   "metadata": {},
   "source": [
    "## Please note the following:\n",
    "\n",
    "Points\n",
    "* The exam is 100 points.\n",
    "* You have 120 minutes to complete the exam.\n",
    "* For every minute late you submit the exam, you will lose one point.\n",
    "\n",
    "\n",
    "Submission\n",
    "* You will upload your solution to the `Midterm 2` assignment on Canvas, where you downloaded this. \n",
    "* Be sure to **submit** on Canvas, not just **save** on Canvas.\n",
    "* Your submission should be readable, (the graders can understand your answers.)\n",
    "* Your submission should **include all code used in your analysis in a file format that the code can be executed.** \n",
    "\n",
    "Rules\n",
    "* The exam is open-material, closed-communication.\n",
    "* You do not need to cite material from the course github repo - you are welcome to use the code posted there without citation.\n",
    "\n",
    "Advice\n",
    "* If you find any question to be unclear, state your interpretation and proceed. We will only answer questions of interpretation if there is a typo, error, etc.\n",
    "* The exam will be graded for partial credit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624f27b1",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "**All data files are found in the class github repo, in the `data` folder.**\n",
    "\n",
    "This exam makes use of the following data files:\n",
    "* `midterm_2_data.xlsx`\n",
    "\n",
    "This file contains the following sheets:\n",
    "- for Section 2:\n",
    "    * `sector stocks excess returns` - MONTHLY excess returns for 49 sector stocks\n",
    "    * `factors excess returns` - MONTHLY excess returns of AQR factor model from Homework 5\n",
    "- for Section 3:\n",
    "    * `factors excess returns` - MONTHLY excess returns of AQR factor model from Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6e066",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "| Problem | Points |\n",
    "|---------|--------|\n",
    "| 1       | 25     |\n",
    "| 2       | 40     |\n",
    "| 3       | 35     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb2fc26",
   "metadata": {},
   "source": [
    "### Each numbered question is worth 5 points unless otherwise specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81156e8f",
   "metadata": {},
   "source": [
    "# 1. Short Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf4bc8",
   "metadata": {},
   "source": [
    "#### No Data Needed\n",
    "\n",
    "These problems do not require any data file. Rather, analyze them conceptually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2ec27",
   "metadata": {},
   "source": [
    "### 1.1.\n",
    "\n",
    "Historically, which pricing factor among the ones we studied has shown a considerable decrease in importance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0286c32",
   "metadata": {},
   "source": [
    "The SMB/Size factor has shown a decrease in importance. This is due to markets becoming more efficient, greater investor awareness, and structural changes to the market where small cap stocks are now more available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c8109",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1.2.\n",
    "\n",
    "True or False: For a given factor model and a set of test assets, the addition of one more factor to that model will surely decrease the cross-sectional MAE. \n",
    "\n",
    "True or False: For a given factor model and a set of test assets, the addition of one more factor to that model will surely decrease the time-series MAE. \n",
    "\n",
    "Along with stating T/F, explain your reasoning for the two statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f282b5",
   "metadata": {},
   "source": [
    "False. Adding an additional factor can introduce noise if the factor isnt capturing significant cross section variation in asset returns, and is not guarenteed to decrease CS MAE\n",
    "\n",
    "False. A new factor could be irrelevant, introduce multicollinearity, and/or overfit the model, which would not decrease TS MAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c00026",
   "metadata": {},
   "source": [
    "### 1.3.\n",
    "\n",
    "Consider the scenario in which you are helping two people with investments.\n",
    "\n",
    "* The young person has a 50 year investment horizon.\n",
    "* The elderly person has a 10 year investment horizon.\n",
    "* Both individuals have the same portfolio holdings.\n",
    "\n",
    "State who has the more certain cumulative return and explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc7662",
   "metadata": {},
   "source": [
    "The elderly person with a 10 year horizon. Over a short time period, the effects of continious compounding is limited, and the cumulative return will be more predictable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d4d72",
   "metadata": {},
   "source": [
    "### 1.4.\n",
    "\n",
    "Suppose we find that the 10-year bond yield works well as a new pricing factor, along with `MKT`.\n",
    "\n",
    "Consider two ways of building this new factor.\n",
    "1. Directly use the index of 10-year yields, `YLD`\n",
    "1. Construct a Fama-French style portfolio of equities, `FFYLD`. (Rank all the stocks by their correlation to bond yield changes, and go long the highest ranked and shor tthe lowest ranked.)\n",
    "\n",
    "Could you test the model with `YLD` and the model with `FFYLD` in the exact same ways? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6edeaf3",
   "metadata": {},
   "source": [
    "No, we would not test these the same way. YLD is a direct measure of bond yields and should be tested as an individual variable in regressions, while FFYLD is a portfolio based factor and needs to be included/tested similar to how we test other factors in the Fama French model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf2d238",
   "metadata": {},
   "source": [
    "### 1.5.\n",
    "\n",
    "Suppose we implement a momentum strategy on cryptocurrencies rather than US stocks.\n",
    "\n",
    "Conceptually speaking, but specific to the context of our course discussion, how would the risk profile differ from the momentum strategy of US equities?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93260254",
   "metadata": {},
   "source": [
    "Cryptocurrencies incur much higher volatility, price swings, and less established momentum premiums. This would lead to a far less stable momentum factor. This strategy would also be suseptcible to other issues like low liquidity, larger tail risk, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ce7d4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8a354",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2. Pricing and Tangency Portfolio\n",
    "\n",
    "You work in a hedge fund that believes that the AQR 4-Factor Model (present in Homework 5) is the perfect pricing model for stocks.\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\tilde{r}^i \\right] = \\beta^{i,\\text{MKT}} \\mathbb{E} \\left[ \\tilde{f}_{\\text{MKT}} \\right] + \\beta^{i,\\text{HML}} \\mathbb{E} \\left[ \\tilde{f}_{\\text{HML}} \\right] + \\beta^{i,\\text{RMW}} \\mathbb{E} \\left[ \\tilde{f}_{\\text{RMW}} \\right] + \\beta^{i,\\text{UMD}} \\mathbb{E} \\left[ \\tilde{f}_{\\text{UMD}} \\right]\n",
    "$$\n",
    "\n",
    "The factors are available in the sheet `factors excess returns`.\n",
    "\n",
    "The hedge fund invests in sector-tracking ETFs available in the sheet `sectors excess returns`. You are to allocate into these sectors according to a mean-variance optimization with...\n",
    "\n",
    "* regularization: elements outside the diagonal covariance matrix divided by 2.\n",
    "* modeled risk premia: expected excess returns given by the factor model rather than just using the historic sample averages.\n",
    "\n",
    "You are to train the portfolio and test out-of-sample. The timeframes should be:\n",
    "* Training timeframe: Jan-2018 to Dec-2022.\n",
    "* Testing timeframe: Jan-2023 to most recent observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db465bc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.1.\n",
    "(8pts)\n",
    "\n",
    "Calculate the model-implied expected excess returns of every asset.\n",
    "\n",
    "The time-series estimations should...\n",
    "* NOT include an intercept. (You assume the model holds perfectly.)\n",
    "* use data from the `training` timeframe.\n",
    "\n",
    "With the time-series estimates, use the `training` timeframe's sample average of the factors as the factor premia. Together, this will give you the model-implied risk premia, which we label as\n",
    "$$\n",
    "\\lambda_i := \\mathbb{E}[\\tilde{r}_i]\n",
    "$$\n",
    "\n",
    "* Store $\\lambda_i$ and $\\boldsymbol{\\beta}^i$ for each asset.\n",
    "* Print $\\lambda_i$ for `Agric`, `Food`, `Soda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a80c08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agric    0.003655\n",
      "Food     0.005454\n",
      "Soda     0.007336\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "factors_rts = pd.read_excel(\"midterm_2_data.xlsx\", sheet_name='factors excess returns').set_index('date')\n",
    "sector_rts = pd.read_excel(\"midterm_2_data.xlsx\", sheet_name='sector excess returns').set_index('date')\n",
    "\n",
    "start = '2018-01-01'\n",
    "end = '2022-12-31'\n",
    "\n",
    "factors_train = factors_rts.loc[start:end]\n",
    "sector_train = sector_rts.loc[start:end]\n",
    "\n",
    "factor_names = ['MKT', 'HML', 'RMW', 'UMD']\n",
    "factor_premia = factors_train.mean()\n",
    "\n",
    "betas = pd.DataFrame(index=sector_train.columns, columns=factor_names)\n",
    "lambdas = pd.Series(index=sector_train.columns, dtype=np.float64)\n",
    "\n",
    "for asset in sector_train.columns:\n",
    "    y = sector_train[asset].values\n",
    "    X = factors_train.values\n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(X, y)\n",
    "    beta_i = model.coef_\n",
    "    betas.loc[asset] = beta_i\n",
    "    lambdas[asset] = np.dot(beta_i, factor_premia.values)\n",
    "\n",
    "print(lambdas[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80c6b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.2.\n",
    "\n",
    "Use the expected excess returns derived from (2.1) with the **regularized** covariance matrix to calculate the weights of the tangency portfolio.\n",
    "\n",
    "- Use the covariance matrix only for `training` timeframe.\n",
    "- Calculate and store the vector of weights for all the assets.\n",
    "- Return the weights of the tangency portfolio for `Agric`, `Food`, `Soda`.\n",
    "\n",
    "$$\n",
    "\\textbf{w}_{t} = \\dfrac{\\tilde{\\Sigma}^{-1} \\bm{\\lambda}}{\\bm{1}' \\tilde{\\Sigma}^{-1} \\bm{\\lambda}}\n",
    "$$\n",
    "\n",
    "Where $\\tilde{\\Sigma}^{-1}$ is the regularized covariance-matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09887cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agric   -0.030723\n",
      "Food     0.015320\n",
      "Soda     0.132944\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "cov_matrix = sector_train.cov()\n",
    "cov_matrix_reg = cov_matrix.copy()\n",
    "\n",
    "#chat GPT prompt: help me create a regularzied covariance matrix\n",
    "\n",
    "for i in cov_matrix_reg.columns:\n",
    "    for j in cov_matrix_reg.index:\n",
    "        if i != j:\n",
    "            cov_matrix_reg.loc[i, j] /= 2\n",
    "\n",
    "inv_cov_matrix = np.linalg.inv(cov_matrix_reg)\n",
    "\n",
    "#chat GPT: how to get tangency portfolio\n",
    "\n",
    "onevec = np.ones(len(lambdas))\n",
    "numerator = inv_cov_matrix.dot(lambdas.values)\n",
    "denominator = onevec.T.dot(numerator)\n",
    "wghts = numerator / denominator\n",
    "\n",
    "weights_df = pd.Series(wghts, index=sector_train.columns)\n",
    "\n",
    "print(weights_df[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c171c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.3.\n",
    "\n",
    "Evaluate the performance of this allocation in the `testing` period. Report the **annualized**\n",
    "- mean\n",
    "- vol\n",
    "- Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f64841a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annualized Mean Return: 0.1812\n",
      "Annualized Volatility: 0.1195\n",
      "Annualized Sharpe Ratio: 1.5155\n"
     ]
    }
   ],
   "source": [
    "start_test = '2023-01-01'\n",
    "sector_test = sector_rts.loc[start_test:]\n",
    "\n",
    "sector_test = sector_test[weights_df.index]\n",
    "portfolio_returns_test = sector_test.dot(weights_df)\n",
    "mean_rt = portfolio_returns_test.mean()\n",
    "vol_rt = portfolio_returns_test.std()\n",
    "\n",
    "mean_rt_ann = mean_rt * 12\n",
    "vol_rt_ann = vol_rt * np.sqrt(12)\n",
    "sharpe_ratio = mean_rt_ann / vol_rt_ann\n",
    "\n",
    "\n",
    "#chat GPT: create print statements for these variables\n",
    "\n",
    "print(f\"Annualized Mean Return: {mean_rt_ann:.4f}\")\n",
    "print(f\"Annualized Volatility: {vol_rt_ann:.4f}\")\n",
    "print(f\"Annualized Sharpe Ratio: {sharpe_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6f8bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.4.\n",
    "\n",
    "(7pts)\n",
    "\n",
    "Construct the same tangency portfolio as in `2.2` but with one change:\n",
    "* replace the risk premia of the assets, (denoted $\\lambda_i$) with the sample averages of the excess returns from the `training` set.\n",
    "\n",
    "So instead of using $\\lambda_i$ suggested by the factor model (as in `2.1-2.3`) you're using sample averages for $\\lambda_i$.\n",
    "\n",
    "- Return the weights of the tangency portfolio for `Agric`, `Food`, `Soda`.\n",
    "\n",
    "Evaluate the performance of this allocation in the `testing` period. Report the **annualized**\n",
    "- mean\n",
    "- vol\n",
    "- Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c02450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tangency Portfolio Weights (Using Sample Averages) for Agric, Food, Soda:\n",
      "Agric    0.14409\n",
      "Food    -0.06981\n",
      "Soda     0.32268\n",
      "dtype: float64\n",
      "Tangency Portfolio Performance (Using Sample Averages) in Testing Period:\n",
      "Annualized Mean Return: 0.1768\n",
      "Annualized Volatility: 0.1530\n",
      "Annualized Sharpe Ratio: 1.1555\n"
     ]
    }
   ],
   "source": [
    "lambda_avg = sector_train.mean()\n",
    "num_avg = inv_cov_matrix.dot(lambda_avg.values)\n",
    "denom_avg = onevec.T.dot(num_avg)\n",
    "wghts_sample = num_avg / denom_avg\n",
    "weights_df_sample = pd.Series(wghts_sample, index=sector_train.columns)\n",
    "\n",
    "#Chat gpt: print these\n",
    "\n",
    "print(\"Tangency Portfolio Weights (Using Sample Averages) for Agric, Food, Soda:\")\n",
    "print(weights_df_sample[:3])\n",
    "\n",
    "sector_test = sector_test[weights_df_sample.index]\n",
    "\n",
    "portfolio_returns_test_sample = sector_test.dot(weights_df_sample)\n",
    "mean_rt_sample = portfolio_returns_test_sample.mean()\n",
    "vol_rt_sample = portfolio_returns_test_sample.std()\n",
    "\n",
    "mean_rt_ann_sample = mean_rt_sample * 12\n",
    "vol_rt_ann_sample = vol_rt_sample * np.sqrt(12)\n",
    "sharpe_ratio_sample = mean_rt_ann_sample / vol_rt_ann_sample\n",
    "\n",
    "#chat gpt: print these\n",
    "\n",
    "print(\"Tangency Portfolio Performance (Using Sample Averages) in Testing Period:\")\n",
    "print(f\"Annualized Mean Return: {mean_rt_ann_sample:.4f}\")\n",
    "print(f\"Annualized Volatility: {vol_rt_ann_sample:.4f}\")\n",
    "print(f\"Annualized Sharpe Ratio: {sharpe_ratio_sample:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c172cbe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.5.\n",
    "\n",
    "Which allocation performed better in the `testing` period: the allocation based on premia from the factor model or from the sample averages?\n",
    "\n",
    "Why might this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad107030",
   "metadata": {},
   "source": [
    "The allocation based on premia from the factor model performs better, as it has a Sharpe Ratio of 1.5155 vs 1.1555. This may be because factor models are less prone to overfitting and account for systematic risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af10554",
   "metadata": {},
   "source": [
    "### 2.6.\n",
    "Suppose you now want to build a tangency portfolio solely from the factors, without using the sector ETFs.\n",
    "\n",
    "- Calculate the weights of the tangency portfolio using `training` data for the factors.\n",
    "- Again, regularize the covariance matrix of factor returns by dividing off-diagonal elements by 2.\n",
    "\n",
    "Report, in the `testing` period, the factor-based tangency stats **annualized**...\n",
    "- mean\n",
    "- vol\n",
    "- Sharpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "605f9414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor-Based Tangency Portfolio Performance in Testing Period:\n",
      "Annualized Mean Return: 0.0624\n",
      "Annualized Volatility: 0.0582\n",
      "Annualized Sharpe Ratio: 1.0719\n"
     ]
    }
   ],
   "source": [
    "cov_factors = factors_train.cov()\n",
    "cov_factors_reg = cov_factors.copy()\n",
    "for i in cov_factors_reg.columns:\n",
    "    for j in cov_factors_reg.index:\n",
    "        if i != j:\n",
    "            cov_factors_reg.loc[i, j] /= 2\n",
    "inv_cov_factors = np.linalg.inv(cov_factors_reg)\n",
    "\n",
    "#chat gpt: how to find premias\n",
    "\n",
    "num_factors = inv_cov_factors.dot(factor_premia.values)\n",
    "denominator_factors = np.ones(len(factor_premia)).T.dot(num_factors)\n",
    "wghts_factors = num_factors / denominator_factors\n",
    "weights_factors_df = pd.Series(wghts_factors, index=factor_names)\n",
    "\n",
    "factors_test = factors_rts.loc[start_test:]\n",
    "factor_returns_test = factors_test\n",
    "portfolio_returns_test_factors = factor_returns_test.dot(weights_factors_df)\n",
    "\n",
    "mean_rt_factors = portfolio_returns_test_factors.mean()\n",
    "vol_rt_factors = portfolio_returns_test_factors.std()\n",
    "mean_rt_ann_factors = mean_rt_factors * 12\n",
    "vol_rt_ann_factors = vol_rt_factors * np.sqrt(12)\n",
    "sharpe_ratio_factors = mean_rt_ann_factors / vol_rt_ann_factors\n",
    "\n",
    "#chat gpt: print these variables\n",
    "\n",
    "print(\"Factor-Based Tangency Portfolio Performance in Testing Period:\")\n",
    "print(f\"Annualized Mean Return: {mean_rt_ann_factors:.4f}\")\n",
    "print(f\"Annualized Volatility: {vol_rt_ann_factors:.4f}\")\n",
    "print(f\"Annualized Sharpe Ratio: {sharpe_ratio_factors:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d96085",
   "metadata": {},
   "source": [
    "### 2.7.\n",
    "\n",
    "Based on the hedge fund's beliefs, would you prefer to use the ETF-based tangency or the factor-based tangency portfolio? Explain your reasoning. Note that you should answer based on broad principles and not on the particular estimation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf4d3f",
   "metadata": {},
   "source": [
    "I would prefer to use the factor based tangency portfolio. Investing in the factors closely aligns with the hedge fund's philosiphy, offers exposure to systematic risk premia without sector specific risks, and offers better risk adjuted returns by focusing on sources of expected return the model identifies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad3346a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df41e0e",
   "metadata": {},
   "source": [
    "# 3. Long-Run Returns\n",
    "\n",
    "For this question, use only the sheet `factors excess returns`.\n",
    "\n",
    "Suppose we want to measure the long run returns of various pricing factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7254eb5",
   "metadata": {},
   "source": [
    "### 3.1.\n",
    "\n",
    "Turn the data into log returns.\n",
    "- Display the first 5 rows of the data.\n",
    "\n",
    "Using these log returns, report the **annualized**\n",
    "* mean\n",
    "* vol\n",
    "* Sharpe\n",
    "\n",
    "### 3.2.\n",
    "\n",
    "Consider 15-year cumulative log excess returns. Following the assumptions and modeling of Lecture 6, report the following 15-year stats:\n",
    "- mean\n",
    "- vol\n",
    "- Sharpe\n",
    "\n",
    "How do they compare to the estimated stats (1-year horizon) in `3.1`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa73e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 MKT       HML       RMW       UMD\n",
      "date                                              \n",
      "1980-01-01  0.053636  0.017349 -0.017146  0.072786\n",
      "1980-02-01 -0.012275  0.006081  0.000400  0.075849\n",
      "1980-03-01 -0.138113 -0.010151  0.014494 -0.100373\n",
      "1980-04-01  0.038932  0.010544 -0.021224 -0.004309\n",
      "1980-05-01  0.051263  0.003793  0.003394 -0.011263\n",
      "         Mean  Volatility    Sharpe\n",
      "MKT  0.073549    0.158841  0.463033\n",
      "HML  0.019770    0.109782  0.180081\n",
      "RMW  0.043540    0.083573  0.520979\n",
      "UMD  0.050095    0.160433  0.312248\n"
     ]
    }
   ],
   "source": [
    "log_factor_rts = np.log(1 + factors_rts)\n",
    "print(log_factor_rts.head())\n",
    "\n",
    "mean_log_rts = log_factor_rts.mean()\n",
    "vol_log_rts = log_factor_rts.std()\n",
    "\n",
    "mean_log_rts_annualized = mean_log_rts * 12\n",
    "vol_log_rts_annualized = vol_log_rts * np.sqrt(12)\n",
    "\n",
    "sharpe_ratio_log = mean_log_rts_annualized / vol_log_rts_annualized\n",
    "\n",
    "#chat gpt: put these in a dataframe\n",
    "\n",
    "performance_df = pd.DataFrame({\n",
    "    'Mean': mean_log_rts_annualized,\n",
    "    'Volatility': vol_log_rts_annualized,\n",
    "    'Sharpe': sharpe_ratio_log\n",
    "})\n",
    "print(performance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f71844e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Mean (15y)  Volatility (15y)  Sharpe (15y)\n",
      "MKT    1.103228          0.615188      1.793318\n",
      "HML    0.296544          0.425182      0.697452\n",
      "RMW    0.653096          0.323676      2.017743\n",
      "UMD    0.751422          0.621353      1.209332\n"
     ]
    }
   ],
   "source": [
    "T = 15\n",
    "\n",
    "mean_cum_return_15y = mean_log_rts_annualized * T\n",
    "vol_cum_return_15y = vol_log_rts_annualized * np.sqrt(T)\n",
    "sharpe_ratio_15y = mean_cum_return_15y / vol_cum_return_15y\n",
    "\n",
    "#chat gpt: put these in a data frame\n",
    "\n",
    "performance_15y_df = pd.DataFrame({\n",
    "    'Mean (15y)': mean_cum_return_15y,\n",
    "    'Volatility (15y)': vol_cum_return_15y,\n",
    "    'Sharpe (15y)': sharpe_ratio_15y\n",
    "})\n",
    "print(performance_15y_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eacfbc",
   "metadata": {},
   "source": [
    "Over a longer time period, we get the benefits of time diversification, and our Sharpe ratios improve across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0721e5",
   "metadata": {},
   "source": [
    "### 3.3.\n",
    "\n",
    "What is the probability that momentum factor has a negative mean excess return over the next \n",
    "* single period?\n",
    "* 15 years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a3e6786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that UMD has negative mean excess return over next year: 0.3774\n",
      "Probability that UMD has negative mean excess return over next 15 years: 0.1133\n"
     ]
    }
   ],
   "source": [
    "#chat gpt: show me how to calculate factor performance probability\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "mean_umd = mean_log_rts_annualized['UMD']\n",
    "vol_umd = vol_log_rts_annualized['UMD']\n",
    "\n",
    "z_single = (-mean_umd) / vol_umd\n",
    "prob_negative_single = norm.cdf(z_single)\n",
    "\n",
    "mean_cum_umd_15y = mean_umd * T\n",
    "vol_cum_umd_15y = vol_umd * np.sqrt(T)\n",
    "z_15y = (-mean_cum_umd_15y) / vol_cum_umd_15y\n",
    "prob_negative_15y = norm.cdf(z_15y)\n",
    "\n",
    "#chat gpt: print these\n",
    "\n",
    "print(f\"Probability that UMD has negative mean excess return over next year: {prob_negative_single:.4f}\")\n",
    "print(f\"Probability that UMD has negative mean excess return over next 15 years: {prob_negative_15y:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09963cc",
   "metadata": {},
   "source": [
    "### 3.4.\n",
    "\n",
    "Recall from the case that momentum has been underperforming since 2009. \n",
    "\n",
    "Using data from 2009 to present, what is the probability that momentum *outperforms* the market factor over the next\n",
    "* period?\n",
    "* 15 years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26f7764f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that UMD outperforms MKT over next year: 0.2780\n",
      "Probability that UMD outperforms MKT over next 15 years: 0.0113\n"
     ]
    }
   ],
   "source": [
    "start = '2009-01-01'\n",
    "data_2009 = factors_rts.loc[start:]\n",
    "log_data = np.log(1 + data_2009 / 100)\n",
    "log_excess = log_data['UMD'] - log_data['MKT']\n",
    "\n",
    "mean_excess = log_excess.mean()\n",
    "vol_excess = log_excess.std()\n",
    "\n",
    "mean_annual = mean_excess * 12\n",
    "vol_annual = vol_excess * np.sqrt(12)\n",
    "\n",
    "z_single = -mean_annual / vol_annual\n",
    "prob_under_single = norm.cdf(z_single)\n",
    "prob_out_single = 1 - prob_under_single\n",
    "\n",
    "mean_15y = mean_annual * T\n",
    "vol_15y = vol_annual * np.sqrt(T)\n",
    "z_15y = -mean_15y / vol_15y\n",
    "prob_under_15y = norm.cdf(z_15y)\n",
    "prob_out_15y = 1 - prob_under_15y\n",
    "\n",
    "#chat gpt: print these results\n",
    "\n",
    "print(f\"Probability that UMD outperforms MKT over next year: {prob_out_single:.4f}\")\n",
    "print(f\"Probability that UMD outperforms MKT over next 15 years: {prob_out_15y:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61981e79",
   "metadata": {},
   "source": [
    "### 3.5.\n",
    "Conceptually, why is there such a discrepancy between this probability for 1 period vs. 15 years?\n",
    "\n",
    "What assumption about the log-returns are we making when we use this technique to estimate underperformance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27595fe",
   "metadata": {},
   "source": [
    "Since mean returns scale with time, and volatility scales with the square root of time, cumulative mean return trends higher than cumulative volatility, reducing the odds of \"underperformance\" as time grows. We are also assuming that log returns are normally distributed and independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b9e1da",
   "metadata": {},
   "source": [
    "### 3.6.\n",
    "\n",
    "Using your previous answers, explain what is meant by time diversification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b400aa8",
   "metadata": {},
   "source": [
    "Time diversification is the concept that holding investments over long periods reduces the probability of negative cumulative returns. Given the facts about how returns and volatility scale in my prevous answer, this means Sharpe ratio grows over time, too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46869f",
   "metadata": {},
   "source": [
    "### 3.7.\n",
    "\n",
    "Is the probability that `HML` and `UMD` both have negative cumulative returns over the next year higher or lower than the probability that `HML` and `MKT` both have negative cumulative returns over the next year?\n",
    "\n",
    "Answer conceptually, but specifically. (No need to calculate the specific probabilities.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb183e",
   "metadata": {},
   "source": [
    "HML and UMD both having negative cumulative returns is a LOWER probability than HML and MKT both having negative cumulative returns. Since we have observed HML and UMD to have little or negative correlations, while we have seen the opposite for HML and MKT, we are more likely to see opposite returns out oh HML and UMD than similar returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8eda25",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
