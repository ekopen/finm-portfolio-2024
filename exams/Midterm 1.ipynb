{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559895d2",
   "metadata": {},
   "source": [
    "# Midterm 1\n",
    "\n",
    "## FINM 36700 - 2024\n",
    "\n",
    "### UChicago Financial Mathematics\n",
    "\n",
    "* Mark Hendricks\n",
    "* hendricks@uchicago.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cde8d3",
   "metadata": {},
   "source": [
    "# Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc273c1a",
   "metadata": {},
   "source": [
    "## Please note the following:\n",
    "\n",
    "Points\n",
    "* The exam is `100` points.\n",
    "* You have `125` minutes to complete the exam.\n",
    "* For every minute late you submit the exam, you will lose one point.\n",
    "\n",
    "\n",
    "Submission\n",
    "* You will upload your solution to the `Midterm 1` assignment on Canvas, where you downloaded this. (Be sure to **submit** on Canvas, not just **save** on Canvas.\n",
    "* Your submission should be readable, (the graders can understand your answers,)\n",
    "* and it should **include all code used in your analysis in a file format that the code can be executed.** \n",
    "\n",
    "Rules\n",
    "* The exam is open-material, closed-communication.\n",
    "* You do not need to cite material from the course github repo--you are welcome to use the code posted there without citation.\n",
    "\n",
    "Advice\n",
    "* If you find any question to be unclear, state your interpretation and proceed. We will only answer questions of interpretation if there is a typo, error, etc.\n",
    "* The exam will be graded for partial credit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624f27b1",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "**All data files are found in the class github repo, in the `data` folder.**\n",
    "\n",
    "This exam makes use of the following data files:\n",
    "* `midterm_1_data.xlsx`\n",
    "\n",
    "This file has sheets for...\n",
    "* `stocks excess returns` - excess returns of the 14 biggest companies in the S&P.\n",
    "* `proshares excess returns` - excess returns of ETFs and indexes from the Proshares case study.\n",
    "* `fx carry excess returns` - excess returns from FX products.\n",
    "\n",
    "Note the data is **monthly** for the first two sheets (stocks and proshares). Any annualizations for those two sheets should use `12` months in a year. Annualization for the third sheet (fx carry excess returns) is explained in section 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6e066",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "| Problem | Points |\n",
    "|---------|--------|\n",
    "| 1       | 15     |\n",
    "| 2       | 25     |\n",
    "| 3       | 35     |\n",
    "| 4       | 25     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb2fc26",
   "metadata": {},
   "source": [
    "### Each numbered question is worth 5 points unless otherwise specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ce7d4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a39225-f8d7-4a42-a675-fb9716e55932",
   "metadata": {},
   "source": [
    "# 1. Short Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d44ea-9d06-4bba-ac24-f19e9376c8d6",
   "metadata": {},
   "source": [
    "#### No Data Needed\n",
    "\n",
    "These problems do not require any data file. Rather, analyze them conceptually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668aaba8-6dcf-4700-9714-36111e1c67af",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "#### (10pts)\n",
    "\n",
    "In the mean-variance optimization of `homework 1`, suppose we found the mean excess return of TIPS is 4% annualized.\n",
    "\n",
    "Explain--conceptually--how each of the following would have impacted the new (with TIPS) MV solution.\n",
    "* TIPS is found to have correlation of 0% to `IEF` and 0% to SPY.\n",
    "* TIPS is found to have correlation of 100% to `IEF`.\n",
    "\n",
    "Would it be possible for TIPS to have been found to have 0% correlation to every other asset in `homework 1`? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8405651",
   "metadata": {},
   "source": [
    "TIPS already had quite a low correlation to our portfolio, and that is why it was so valuable, because it reduced the risk profile of our portfolio via diversification. If it would have had a 0% correlation, then that would have made it even more valuable. However, if it was highly corelated to IEF, than we would not have gained as much benefit from it. IEF is the ticker for 10 year treasury bonds, so including another highly correlated bond wouldn't have been very valuable unless TIPS had a large return. It is very unlikely TIPS would have a 0% correlation to the assets. Since there were already bonds in the portfolio, it was bond to have some sort of correlation with them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe910811-eddc-48ae-9e3c-2b99e6c62dec",
   "metadata": {},
   "source": [
    "### 2. \n",
    "\n",
    "Depending on the application, one may or may not choose to include an intercept term in a linear factor decomposition of an assetâ€™s returns. In what circumstances would I prefer to include an intercept, and in what circumstances would I not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0bb916",
   "metadata": {},
   "source": [
    "An intercept makes sense when the intercept is investible. For example, if the intercept is the interest free rate, then we can absolutely use it. However, in the case when it is not investible, like in our example about replicating hedge fund returns where the intercept represented the alpha hedge funds generate, we cannot easily invest in it, and should not include it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b33d9-8628-4bf5-a662-73da5dfe0ace",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ee6f4",
   "metadata": {},
   "source": [
    "# 2. Portfolio Allocation\n",
    "\n",
    "For this question you will only use data from the sheet `stocks excess returns`.\n",
    "\n",
    "It contains excess returns for the 14 largest stocks in the S&P."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d1b59",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "Calculate the tangency portfolio from the start of the sample to December of 2018 (to 2018-12-31), which we call in-sample period. Use the following methods:\n",
    "- Traditional tangency portfolio.\n",
    "- Regularized tangency portfolio (divide by 2 every element outside of the diagonal in the covariance matrix prior to the calculation).\n",
    "\n",
    "Return:\n",
    "- The weights of each asset for the traditional tangency portfolio and the regularized tangency portfolio.\n",
    "- The sum of absolute values of the weights for the traditional tangency portfolio and the regularized tangency portfolio:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} |w_i|\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8af20b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fc8dc6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>BRK-B</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>JPM</th>\n",
       "      <th>LLY</th>\n",
       "      <th>META</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>UNH</th>\n",
       "      <th>V</th>\n",
       "      <th>XOM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-06-30</th>\n",
       "      <td>0.010943</td>\n",
       "      <td>0.072609</td>\n",
       "      <td>0.050116</td>\n",
       "      <td>-0.001270</td>\n",
       "      <td>0.082263</td>\n",
       "      <td>0.077918</td>\n",
       "      <td>0.047953</td>\n",
       "      <td>0.050766</td>\n",
       "      <td>0.048052</td>\n",
       "      <td>0.111916</td>\n",
       "      <td>0.060768</td>\n",
       "      <td>0.052919</td>\n",
       "      <td>0.073267</td>\n",
       "      <td>0.088352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-31</th>\n",
       "      <td>0.045822</td>\n",
       "      <td>0.021677</td>\n",
       "      <td>0.018121</td>\n",
       "      <td>0.091196</td>\n",
       "      <td>0.024570</td>\n",
       "      <td>0.015958</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>-0.301929</td>\n",
       "      <td>-0.036613</td>\n",
       "      <td>-0.020260</td>\n",
       "      <td>-0.123682</td>\n",
       "      <td>-0.126667</td>\n",
       "      <td>0.044002</td>\n",
       "      <td>0.014958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08-31</th>\n",
       "      <td>0.093695</td>\n",
       "      <td>0.063985</td>\n",
       "      <td>-0.006075</td>\n",
       "      <td>0.082161</td>\n",
       "      <td>-0.017188</td>\n",
       "      <td>0.031486</td>\n",
       "      <td>0.031399</td>\n",
       "      <td>-0.168306</td>\n",
       "      <td>0.052556</td>\n",
       "      <td>0.036008</td>\n",
       "      <td>0.039935</td>\n",
       "      <td>0.062649</td>\n",
       "      <td>-0.004849</td>\n",
       "      <td>0.011533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-09-30</th>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.024450</td>\n",
       "      <td>0.045849</td>\n",
       "      <td>0.101397</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.090012</td>\n",
       "      <td>0.055749</td>\n",
       "      <td>0.199417</td>\n",
       "      <td>-0.034312</td>\n",
       "      <td>-0.049099</td>\n",
       "      <td>0.026730</td>\n",
       "      <td>0.024657</td>\n",
       "      <td>0.047099</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-31</th>\n",
       "      <td>-0.107527</td>\n",
       "      <td>-0.084192</td>\n",
       "      <td>-0.020903</td>\n",
       "      <td>-0.098271</td>\n",
       "      <td>0.027789</td>\n",
       "      <td>0.037321</td>\n",
       "      <td>0.025805</td>\n",
       "      <td>-0.025320</td>\n",
       "      <td>-0.040922</td>\n",
       "      <td>-0.101877</td>\n",
       "      <td>-0.039204</td>\n",
       "      <td>0.010720</td>\n",
       "      <td>0.033436</td>\n",
       "      <td>-0.002989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      AMZN     BRK-B     GOOGL       JNJ       JPM  \\\n",
       "date                                                                     \n",
       "2012-06-30  0.010943  0.072609  0.050116 -0.001270  0.082263  0.077918   \n",
       "2012-07-31  0.045822  0.021677  0.018121  0.091196  0.024570  0.015958   \n",
       "2012-08-31  0.093695  0.063985 -0.006075  0.082161 -0.017188  0.031486   \n",
       "2012-09-30  0.002878  0.024450  0.045849  0.101397  0.022030  0.090012   \n",
       "2012-10-31 -0.107527 -0.084192 -0.020903 -0.098271  0.027789  0.037321   \n",
       "\n",
       "                 LLY      META      MSFT      NVDA      TSLA       UNH  \\\n",
       "date                                                                     \n",
       "2012-06-30  0.047953  0.050766  0.048052  0.111916  0.060768  0.052919   \n",
       "2012-07-31  0.026101 -0.301929 -0.036613 -0.020260 -0.123682 -0.126667   \n",
       "2012-08-31  0.031399 -0.168306  0.052556  0.036008  0.039935  0.062649   \n",
       "2012-09-30  0.055749  0.199417 -0.034312 -0.049099  0.026730  0.024657   \n",
       "2012-10-31  0.025805 -0.025320 -0.040922 -0.101877 -0.039204  0.010720   \n",
       "\n",
       "                   V       XOM  \n",
       "date                            \n",
       "2012-06-30  0.073267  0.088352  \n",
       "2012-07-31  0.044002  0.014958  \n",
       "2012-08-31 -0.004849  0.011533  \n",
       "2012-09-30  0.047099  0.047619  \n",
       "2012-10-31  0.033436 -0.002989  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "\n",
    "#get all stock data\n",
    "FILEIN = '../data/midterm_1_data.xlsx'\n",
    "sheet_exrets = 'stocks excess returns'\n",
    "retsx = pd.read_excel(FILEIN, sheet_name=sheet_exrets).set_index('date')\n",
    "\n",
    "retsx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c9bc58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting up in sample and out of sample data\n",
    "retsx_IS = retsx.loc[:'2018']\n",
    "retsx_OOS = retsx.loc['2019':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c265387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=using the function from the practice midterm for the tangency portfolio\n",
    "\n",
    "def tangency_weights(returns, cov_mat = 1):\n",
    "    \n",
    "    if cov_mat ==1:\n",
    "        cov_inv = np.linalg.inv((returns.cov()*12))\n",
    "    else:\n",
    "        cov = returns.cov()\n",
    "        covmat_diag = np.diag(np.diag((cov)))\n",
    "        covmat = cov_mat * cov + (1-cov_mat) * covmat_diag\n",
    "        cov_inv = np.linalg.inv((covmat*12))  \n",
    "        \n",
    "    # ones = np.ones(returns.columns[1:].shape) \n",
    "    ones = np.ones((returns.shape[1], 1))\n",
    "\n",
    "    # mu = returns.mean()*12\n",
    "    mu = returns.mean().values.reshape(-1, 1)  # Convert mu to a column vector\n",
    "\n",
    "    # print(\"ones shape:\", ones.shape)\n",
    "    # print(\"cov_inv shape:\", cov_inv.shape)\n",
    "    # print(\"mu shape:\", mu.shape)\n",
    "    \n",
    "     # scaling = 1/(np.transpose(ones) @ cov_inv @ mu)\n",
    "    scaling = 1 / (ones.T @ cov_inv @ mu)  # Transpose ones to make it a row vector\n",
    "    \n",
    "    tangent_return = scaling*(cov_inv @ mu)\n",
    "\n",
    "    # tangency_wts = pd.DataFrame(index = returns.columns[1:], data = tangent_return, columns = ['Tangent Weights'] )\n",
    "    tangency_wts = pd.DataFrame(index = returns.columns[:], data = tangent_return[:], columns = ['Tangent Weights'] )\n",
    "        \n",
    "    return tangency_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d9edc88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tangency</th>\n",
       "      <th>regularized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.127836</td>\n",
       "      <td>-0.014706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>-0.040576</td>\n",
       "      <td>0.03631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRK-B</th>\n",
       "      <td>0.131333</td>\n",
       "      <td>0.109162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0.025968</td>\n",
       "      <td>0.050545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JNJ</th>\n",
       "      <td>0.130408</td>\n",
       "      <td>0.09919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JPM</th>\n",
       "      <td>-0.013929</td>\n",
       "      <td>0.053691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLY</th>\n",
       "      <td>0.352670</td>\n",
       "      <td>0.214949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>META</th>\n",
       "      <td>0.030541</td>\n",
       "      <td>0.034896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>0.137917</td>\n",
       "      <td>0.08997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVDA</th>\n",
       "      <td>0.163501</td>\n",
       "      <td>0.071696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>0.030066</td>\n",
       "      <td>0.036511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNH</th>\n",
       "      <td>0.290230</td>\n",
       "      <td>0.189845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <td>0.306209</td>\n",
       "      <td>0.184558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>-0.416502</td>\n",
       "      <td>-0.156617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tangency regularized\n",
       "AAPL  -0.127836   -0.014706\n",
       "AMZN  -0.040576     0.03631\n",
       "BRK-B  0.131333    0.109162\n",
       "GOOGL  0.025968    0.050545\n",
       "JNJ    0.130408     0.09919\n",
       "JPM   -0.013929    0.053691\n",
       "LLY    0.352670    0.214949\n",
       "META   0.030541    0.034896\n",
       "MSFT   0.137917     0.08997\n",
       "NVDA   0.163501    0.071696\n",
       "TSLA   0.030066    0.036511\n",
       "UNH    0.290230    0.189845\n",
       "V      0.306209    0.184558\n",
       "XOM   -0.416502   -0.156617"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wts = pd.DataFrame(index = retsx_IS.columns, columns=['tangency','regularized'])\n",
    "wts.loc[:,'tangency'] = tangency_weights(retsx_IS, cov_mat = 1).values\n",
    "wts.loc[:,'regularized'] = tangency_weights(retsx_IS, cov_mat = (1/2)).values #regularizing by a factor of 1/2 as indicated in the question\n",
    "wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "03ecdff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tangency       2.197687\n",
       "regularized    1.342647\n",
       "dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chat GPT prompt: get absoulte value of each row in a data frame\n",
    "wts_absolute = wts.abs()\n",
    "sum_absolute = wts_absolute.sum()\n",
    "sum_absolute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da8bdc",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "Calculate the annualized summary statistics (mean, Sharpe, vol) of both portfolios in-sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9f72a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing performance summary code from practice midterm\n",
    "\n",
    "def performance_summary(return_data, annualization = 12):\n",
    "    \"\"\" \n",
    "        Returns the Performance Stats for given set of returns\n",
    "        Inputs: \n",
    "            return_data - DataFrame with Date index and Monthly Returns for different assets/strategies.\n",
    "        Output:\n",
    "            summary_stats - DataFrame with annualized mean return, vol, sharpe ratio. Skewness, Excess Kurtosis, Var (0.5) and\n",
    "                            CVaR (0.5) and drawdown based on monthly returns. \n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(return_data, pd.Series):\n",
    "        return_data = return_data.to_frame('Returns') \n",
    "        \n",
    "    summary_stats = return_data.mean().to_frame('Mean').apply(lambda x: x*annualization)\n",
    "    summary_stats['Volatility'] = return_data.std().apply(lambda x: x*np.sqrt(annualization))\n",
    "    summary_stats['Sharpe Ratio'] = summary_stats['Mean']/summary_stats['Volatility']\n",
    "    \n",
    "    summary_stats['Skewness'] = return_data.skew()\n",
    "    summary_stats['Excess Kurtosis'] = return_data.kurtosis()\n",
    "    summary_stats['VaR (0.05)'] = return_data.quantile(.05, axis = 0)\n",
    "    summary_stats['CVaR (0.05)'] = return_data[return_data <= return_data.quantile(.05, axis = 0)].mean()\n",
    "    summary_stats['Min'] = return_data.min()\n",
    "    summary_stats['Max'] = return_data.max()\n",
    "    \n",
    "    wealth_index = 1000*(1+return_data).cumprod()\n",
    "    previous_peaks = wealth_index.cummax()\n",
    "    drawdowns = (wealth_index - previous_peaks)/previous_peaks\n",
    "\n",
    "    summary_stats['Max Drawdown'] = drawdowns.min()\n",
    "    summary_stats['Peak'] = [previous_peaks[col][:drawdowns[col].idxmin()].idxmax() for col in previous_peaks.columns]\n",
    "    summary_stats['Bottom'] = drawdowns.idxmin()\n",
    "    \n",
    "    recovery_date = []\n",
    "    for col in wealth_index.columns:\n",
    "        prev_max = previous_peaks[col][:drawdowns[col].idxmin()].max()\n",
    "        recovery_wealth = pd.DataFrame([wealth_index[col][drawdowns[col].idxmin():]]).T\n",
    "        recovery_date.append(recovery_wealth[recovery_wealth[col] >= prev_max].index.min())\n",
    "    summary_stats['Recovery'] = recovery_date\n",
    "    \n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "360e869a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>Sharpe Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tangency Portfolio</th>\n",
       "      <td>0.348256</td>\n",
       "      <td>0.134053</td>\n",
       "      <td>2.597896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regularized Portfolio</th>\n",
       "      <td>0.276599</td>\n",
       "      <td>0.114184</td>\n",
       "      <td>2.422392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Mean  Volatility Sharpe Ratio\n",
       "Tangency Portfolio     0.348256    0.134053     2.597896\n",
       "Regularized Portfolio  0.276599    0.114184     2.422392"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tan_statistics = performance_summary(retsx_IS @ wts['tangency'] , 12)\n",
    "reg_statistics = performance_summary(retsx_IS @ wts['regularized'] , 12)\n",
    "\n",
    "summary_df = pd.DataFrame(columns=['Mean', 'Volatility', 'Sharpe Ratio'])\n",
    "selected_columns1 = tan_statistics[['Mean', 'Volatility', 'Sharpe Ratio']]\n",
    "selected_columns1.index = ['Tangency Portfolio']\n",
    "\n",
    "selected_columns2 = reg_statistics[['Mean', 'Volatility', 'Sharpe Ratio']]\n",
    "selected_columns2.index = ['Regularized Portfolio']\n",
    "\n",
    "summary_df = pd.concat([summary_df, selected_columns1])\n",
    "summary_df = pd.concat([summary_df, selected_columns2])\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f7e39",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Use the weights calculated in question (2.2) to produce portfolio returns out-of-sample for both the Traditional and Regularized portfolio (from January 2019 onwards). \n",
    "\n",
    "Report the **last 3 returns** of both portfolios in the out-of-sample (the traditional tangency portfolio and the regularized tangency portfolio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "efcaed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 3 returns:\n",
      "Tangency Portfolio\n",
      "date\n",
      "2024-07-31   -0.016012\n",
      "2024-08-31    0.101241\n",
      "2024-09-30   -0.025162\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Regularized Portfolio\n",
      "date\n",
      "2024-07-31    0.003227\n",
      "2024-08-31    0.064626\n",
      "2024-09-30   -0.016477\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Chat GPT prompt: how to calulcate these weights vs these returns to calcualte a new weighted returns dataframe \n",
    "weighted_returns_tangency = retsx_OOS * wts['tangency'].values\n",
    "weighted_returns_regularized = retsx_OOS * wts['regularized'].values\n",
    "\n",
    "total_weighted_returns_tangency = weighted_returns_tangency.sum(axis=1)\n",
    "total_weighted_returns_regularized = weighted_returns_regularized.sum(axis=1)\n",
    "\n",
    "print(\"Last 3 returns:\")\n",
    "\n",
    "print(\"Tangency Portfolio\")\n",
    "print(total_weighted_returns_tangency.tail(3))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Regularized Portfolio\")\n",
    "print(total_weighted_returns_regularized.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6aa5f9-4201-4600-b8ff-48c9af2ff5c3",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "Report the annualized summary statistics (Mean, Vol and Sharpe) of both portfolios in the out-of-sample.\n",
    "\n",
    "Note: you are using the weights optimized for the in-sample and generating statistics with the out-of-sample returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15f3a7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>Sharpe Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tangency Portfolio</th>\n",
       "      <td>0.321558</td>\n",
       "      <td>0.222352</td>\n",
       "      <td>1.446167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regularized Portfolio</th>\n",
       "      <td>0.268686</td>\n",
       "      <td>0.176334</td>\n",
       "      <td>1.523729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Mean  Volatility Sharpe Ratio\n",
       "Tangency Portfolio     0.321558    0.222352     1.446167\n",
       "Regularized Portfolio  0.268686    0.176334     1.523729"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tan_statistics = performance_summary(retsx_OOS @ wts['tangency'] , 12)\n",
    "reg_statistics = performance_summary(retsx_OOS @ wts['regularized'] , 12)\n",
    "\n",
    "summary_df = pd.DataFrame(columns=['Mean', 'Volatility', 'Sharpe Ratio'])\n",
    "selected_columns1 = tan_statistics[['Mean', 'Volatility', 'Sharpe Ratio']]\n",
    "selected_columns1.index = ['Tangency Portfolio']\n",
    "\n",
    "selected_columns2 = reg_statistics[['Mean', 'Volatility', 'Sharpe Ratio']]\n",
    "selected_columns2.index = ['Regularized Portfolio']\n",
    "\n",
    "summary_df = pd.concat([summary_df, selected_columns1])\n",
    "summary_df = pd.concat([summary_df, selected_columns2])\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147058f-7c1b-4f56-ae94-d8ae77b5a96d",
   "metadata": {},
   "source": [
    "### 5.\n",
    "Which portfolio has better adjusted by risk returns in the out-of-sample? Could there be a mathematical/optimization reason why one portfolio had better adjusted by risk performance? \n",
    "\n",
    "Relate your answer to your findings in question (2.2) (Sum of absolute weights in the traditional and regularized tangency portfolio.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90d8c8",
   "metadata": {},
   "source": [
    "The regularized portfolio has better performance out of sample, which is what we saw in HW1. This is due to the way we \"optimize\" our portfolio. The matrix math ends up causing some weights to be too extreme without regularization, which is why the abs value of weights for the tangency portfolio are higher, causing the model to perform fine in sample but struggle out of sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8eda25",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9f9ca",
   "metadata": {},
   "source": [
    "# 3. Hedging and Replication\n",
    "\n",
    "For this question you will only use data from the sheet `proshares returns`.\n",
    "\n",
    "The following assets excess returns are available in this sheet:\n",
    "\n",
    "- **HDG US Equity**: ProShares Hedge Replication ET\n",
    "- **QAI US Equity**: NYLI Hedge Multi-Strategy Trac\n",
    "- **SPY US Equity**: SPDR S&P 500 ETF Trust\n",
    "- **EEM US Equity**: iShares MSCI Emerging Markets\n",
    "- **EFA US Equity**: iShares MSCI EAFE ETF\n",
    "- **EUO US Equity**: ProShares UltraShort Euro\n",
    "- **IWM US Equity**: iShares Russell 2000 ETF\n",
    "- **SPXU US Equity**: ProShares UltraPro Short S&P 5\n",
    "- **UPRO US Equity**: ProShares UltraPro S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e7cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6e16122",
   "metadata": {},
   "source": [
    "### 1. \n",
    "\n",
    "You work at a hedge fund.\n",
    "\n",
    "Suppose the hedge fund is long $1 million of HDG and wants to hedge the position.\n",
    "\n",
    "A junior analyst suggests that we can hedge our position by looking at some select ETFs, and then taking a position in the ETFs that will offset the risk of our HDG position.\n",
    "\n",
    "They pick QAI, SPY, EEM, UPRO, SPXU, IWM, and EFA.\n",
    "\n",
    "What dollar position would we be taking in each ETF to hedge your HDG position?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917ec2d",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "#### (7pts)\n",
    "What is the gross notional of the hedge?\n",
    "\n",
    "What is the R-squared of the hedge?\n",
    "\n",
    "What do these two statistics indicate about the practical use of this hedge?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc40b82a-d979-42c7-bbca-f7ede2c133f3",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Suppose instead we don't want to hedge our position. We believe that the value of HDG can be *entirely* determined by some combination of the other ETFs. \n",
    "\n",
    "So, you propose the following model:\n",
    "\n",
    "$$\n",
    "HDG_t = \\beta_1 QAI_t + \\beta_2 SPY_t + \\beta_3 EEM_t + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "We think any difference between the value of HDG and the value of the ETFs is a mispricing, and will revert to 0 in the future. We call such a strategy \"trading the residuals\".\n",
    "\n",
    "Therefore, if $\\varepsilon_t > 0$, we should be short HDG and long the basket, and if $\\varepsilon_t < 0$, we should be long HDG and short the basket.\n",
    "\n",
    "Now...\n",
    "* Run the model specified above and report the $\\beta$'s values.\n",
    "* After, create the \"basket\" portfolio, using the $\\beta$'s as weights (they do not need to add up to one). Report the final three values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c71d0-bcf5-47e0-8539-7a8dc7f8e4fc",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "#### (8pts)\n",
    "\n",
    "Construct the strategy indicated by the approach in the previous problem.\n",
    "\n",
    "For a given period $t$:\n",
    "- if the $\\varepsilon_t \\leq 0$ (is negative or equal to 0), you should be long HDG 200% in HDG and short 100% in the basket portfolio **in period $t+1$**.\n",
    "- if the $\\varepsilon_t > 0$ (is positive), you should be long 200% in the basket portfolio and short 100% **in HDG in period $t+1$**.\n",
    "\n",
    "Do not worry about the look forward bias: in this scenario, you should run the model only once with the entire dataset and define your $\\varepsilon_t$ for any $t$ also considering the model that has acess to data in $t+1, t+2, ...$ to make the calculation.  \n",
    "\n",
    "Report the annualized summary statistics of this strategy (Mean, Vol and Sharpe)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397b033-f586-49bf-9763-489276723d8a",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "On a different matter, we are now studying QAI and want to track (replicate) it using the other available ETFs.\n",
    "\n",
    "Use an intercept and report:\n",
    "\n",
    "- $\\beta$ (and the sum of $\\beta$'s absolute value).\n",
    "- $\\alpha$ and Information Ratio.\n",
    "- $R^2$.\n",
    "- Correlation matrix between the assets used to replicate QAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23fbaed-bfd0-4ba3-80b1-10282619aba9",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "Explain how good is your replication, pointing out at least one good or bad argument related to each of the statistics mentioned in the bullet points above (thus, you should have at least 4 arguments)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd7b8ee",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06f50c-cc60-4ee7-9828-bc6bff90c46d",
   "metadata": {},
   "source": [
    "# 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6cde15-77e9-480a-ac34-0ae92694cdab",
   "metadata": {},
   "source": [
    "The data in sheet `fx carry excess returns` has **excess** daily returns for trading currencies.\n",
    "* You **do NOT need** to know anything about FX, currency, or the underlying strategies.\n",
    "* Rather, just take these return series as given.\n",
    "\n",
    "For the problems below, we will **only use** the `JPY` series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827c465-d9dd-4466-bbc7-946e3fce7029",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "Calculate the `1%` VaR as follows...\n",
    "\n",
    "Empirical VaR:\n",
    "* At every point in time, calculate the `1st` quantile of the returns up to that point.\n",
    "* No need to scale the answers.\n",
    "  \n",
    "Report the VaR for the final date of the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f973bfcf-8d52-4465-8513-f393b85393c4",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "Now calculate the `normal VaR` of `JPY` as follows,\n",
    "\n",
    "$$\\text{Normal VaR (1\\%)} = -2.33\\, \\sigma_t$$\n",
    "\n",
    "where $\\sigma_t$ is estimated with\n",
    "* rolling volatility.\n",
    "* using a window of `233` days.\n",
    "* without using a sample mean.\n",
    "\n",
    "Report the VaR for the final `3 days` of the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98805080-a0db-48bd-ab7a-34574b8ace75",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Now calculate the `normal VaR` of `JPY` as follows,\n",
    "\n",
    "$$\\text{Normal VaR (1\\%)} = -2.33\\, \\sigma_t$$\n",
    "\n",
    "where $\\sigma_t$ is estimated with\n",
    "* EWMA volatility\n",
    "* using $\\lambda = 0.94$.\n",
    "* without using a sample mean.\n",
    "\n",
    "Report the VaR for the final `3 days` of the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1778a4f4-0382-4b27-a4bd-5b980fee9f82",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "Make a plot of the three timeseries of your VaR estimates.\n",
    "\n",
    "Succinctly point out the pros / cons of these approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766661e3-af58-4918-ba7e-e9aba15333da",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "What statistic do we use to judge the performance of a VaR model?\n",
    "\n",
    "Estimate and report this statistic across the VaR methods.\n",
    "\n",
    "Which VaR model do you find is best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff0e17f-9126-4f11-99c0-fa9796143021",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
